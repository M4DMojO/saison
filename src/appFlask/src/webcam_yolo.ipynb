{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import json\n",
    "import time\n",
    "from custom_function import get_result_from_yolo_total, draw_bounding_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "model = YOLO(\"../models/yolo_total.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object classes\n",
    "classNames = [\"Apple\", \"Artichoke\", \"Banana\", \"Bell pepper\", \"Broccoli\", \"Carrot\", \"Orange\", \"Pear\", \"Pumpkin\",\n",
    "              \"Strawberry\"\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/season.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "config_dict = {}\n",
    "config_dict['FRUITS'] = {item['_id']: item['name'] for item in data['master_season']}\n",
    "config_dict['MODELS'] = data['model_dict']\n",
    "config_dict['MONTHS'] = { key : value.capitalize() for key, value in data['month_dict'].items()}\n",
    "config_dict['COUNTRIES'] = data['countries_dict']\n",
    "config_dict['SEASONALITY_TO_COLOR'] = data['seasonality_color_dict_bgr']\n",
    "config_dict['FRUIT_SEASONS'] = {\n",
    "    item['_id']: {key: item.get(key, []) for key in item if key.startswith('season_')}\n",
    "    for item in data['master_season']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 (no detections), 20.0ms\n",
      "Speed: 2.0ms preprocess, 20.0ms inference, 23.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.6ms\n",
      "Speed: 2.0ms preprocess, 2.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.7ms\n",
      "Speed: 2.5ms preprocess, 3.7ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.3ms\n",
      "Speed: 2.0ms preprocess, 2.3ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.2ms\n",
      "Speed: 0.9ms preprocess, 2.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.0ms\n",
      "Speed: 0.9ms preprocess, 2.0ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.8ms\n",
      "Speed: 0.9ms preprocess, 2.8ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.3ms\n",
      "Speed: 0.9ms preprocess, 2.3ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.3ms\n",
      "Speed: 0.9ms preprocess, 2.3ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.1ms\n",
      "Speed: 1.4ms preprocess, 3.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 1.1ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.6ms\n",
      "Speed: 1.3ms preprocess, 2.6ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.2ms\n",
      "Speed: 0.9ms preprocess, 2.2ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.4ms\n",
      "Speed: 1.4ms preprocess, 2.4ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 0.9ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.5ms\n",
      "Speed: 1.0ms preprocess, 2.5ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.4ms\n",
      "Speed: 1.0ms preprocess, 3.4ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.5ms\n",
      "Speed: 1.0ms preprocess, 2.5ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.4ms\n",
      "Speed: 1.0ms preprocess, 2.4ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.2ms\n",
      "Speed: 0.9ms preprocess, 2.2ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.0ms\n",
      "Speed: 0.9ms preprocess, 2.0ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 0.9ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.4ms\n",
      "Speed: 1.1ms preprocess, 2.4ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 1.3ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 1.7ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 0.8ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.5ms\n",
      "Speed: 1.1ms preprocess, 2.5ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.2ms\n",
      "Speed: 1.1ms preprocess, 2.2ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 0.9ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 0.9ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.8ms\n",
      "Speed: 0.8ms preprocess, 2.8ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 0.9ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.5ms\n",
      "Speed: 1.0ms preprocess, 2.5ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 0.9ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 0.9ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.9ms\n",
      "Speed: 1.6ms preprocess, 2.9ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 0.9ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 0.9ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 0.8ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 0.9ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.4ms\n",
      "Speed: 0.9ms preprocess, 2.4ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.0ms\n",
      "Speed: 0.8ms preprocess, 2.0ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 0.9ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.0ms\n",
      "Speed: 0.9ms preprocess, 2.0ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.0ms\n",
      "Speed: 1.5ms preprocess, 2.0ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.2ms\n",
      "Speed: 1.1ms preprocess, 2.2ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.0ms\n",
      "Speed: 0.9ms preprocess, 2.0ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.2ms\n",
      "Speed: 0.9ms preprocess, 2.2ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.4ms\n",
      "Speed: 0.9ms preprocess, 2.4ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 0.9ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 1.1ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 1.5ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 0.9ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.3ms\n",
      "Speed: 1.0ms preprocess, 2.3ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 3.2ms\n",
      "Speed: 0.9ms preprocess, 3.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.2ms\n",
      "Speed: 1.0ms preprocess, 2.2ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 1.0ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.0ms\n",
      "Speed: 0.9ms preprocess, 2.0ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.0ms\n",
      "Speed: 0.9ms preprocess, 2.0ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.0ms\n",
      "Speed: 0.9ms preprocess, 2.0ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 1.0ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 0.9ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.0ms\n",
      "Speed: 0.9ms preprocess, 2.0ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.2ms\n",
      "Speed: 0.9ms preprocess, 2.2ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.0ms\n",
      "Speed: 0.9ms preprocess, 2.0ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 0.9ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 0.9ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.0ms\n",
      "Speed: 0.9ms preprocess, 2.0ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.2ms\n",
      "Speed: 1.1ms preprocess, 2.2ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.1ms\n",
      "Speed: 0.9ms preprocess, 2.1ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 2.0ms\n",
      "Speed: 1.0ms preprocess, 2.0ms inference, 0.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 Banana, 2.1ms\n",
      "Speed: 0.9ms preprocess, 2.1ms inference, 184.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m output \u001b[38;5;241m=\u001b[39m get_result_from_yolo_total(results)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(output) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 14\u001b[0m     img_out \u001b[38;5;241m=\u001b[39m \u001b[43mdraw_bounding_boxes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     img_out \u001b[38;5;241m=\u001b[39m img\n",
      "File \u001b[0;32m~/code/BeltranLuc/saison/src/appFlask/src/custom_function.py:189\u001b[0m, in \u001b[0;36mdraw_bounding_boxes\u001b[0;34m(img, config_dict, results)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# Parcourir les résultats de détection\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m--> 189\u001b[0m     confidence \u001b[38;5;241m=\u001b[39m \u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconfidence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m confidence \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMINIMUM_CONFIDENCE\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    192\u001b[0m         fruit_id \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfruit_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/code/BeltranLuc/saison/.venv-saison/lib/python3.12/site-packages/ultralytics/engine/results.py:287\u001b[0m, in \u001b[0;36mResults.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m    273\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    Return a Results object for a specific index of inference results.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m        >>> subset_results = results[1:4]  # Get a slice of results\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__getitem__\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/code/BeltranLuc/saison/.venv-saison/lib/python3.12/site-packages/ultralytics/engine/results.py:359\u001b[0m, in \u001b[0;36mResults._apply\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, k)\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 359\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(r, k, \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/code/BeltranLuc/saison/.venv-saison/lib/python3.12/site-packages/ultralytics/engine/results.py:183\u001b[0m, in \u001b[0;36mBaseTensor.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m    167\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m    Returns a new BaseTensor instance containing the specified indexed elements of the data tensor.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;124;03m        tensor([1, 2, 3])\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_shape)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "\n",
    "# start webcam\n",
    "url = \"http://192.168.1.98:8080//video\" \n",
    "cap = cv2.VideoCapture(url)\n",
    "# cap.set(3, 640) # Pour réduire la résolution à 640x480\n",
    "# cap.set(4, 480)\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    results = model(img)\n",
    "\n",
    "    output = get_result_from_yolo_total(results)\n",
    "    output = [item for item in output if item['confidence'] >= 0.5]\n",
    "    if len(output) > 0:\n",
    "        img_out = draw_bounding_boxes(img, config_dict, results)\n",
    "\n",
    "    else:\n",
    "        img_out = img\n",
    "\n",
    "    cv2.imshow('Video', img_out)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fruit_id': 2,\n",
       "  'confidence': 0.2869498,\n",
       "  'x1': 942,\n",
       "  'y1': 224,\n",
       "  'x2': 1545,\n",
       "  'y2': 520}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Artefact",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
